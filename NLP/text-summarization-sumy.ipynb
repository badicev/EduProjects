{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting sumy\n",
      "  Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
      "Collecting docopt<0.7,>=0.6.1\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: nltk>=3.0.2 in c:\\users\\basak\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sumy) (3.8)\n",
      "Collecting pycountry>=18.2.23\n",
      "  Downloading pycountry-22.3.5.tar.gz (10.1 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: requests>=2.7.0 in c:\\users\\basak\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sumy) (2.28.1)\n",
      "Collecting breadability>=0.1.20\n",
      "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting chardet\n",
      "  Downloading chardet-5.1.0-py3-none-any.whl (199 kB)\n",
      "Collecting lxml>=2.0\n",
      "  Downloading lxml-4.9.2-cp311-cp311-win_amd64.whl (3.8 MB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\basak\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk>=3.0.2->sumy) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in c:\\users\\basak\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk>=3.0.2->sumy) (4.64.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\basak\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk>=3.0.2->sumy) (1.1.1)\n",
      "Requirement already satisfied: click in c:\\users\\basak\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk>=3.0.2->sumy) (8.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\basak\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pycountry>=18.2.23->sumy) (66.0.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\basak\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.7.0->sumy) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\basak\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.7.0->sumy) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\basak\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.7.0->sumy) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\basak\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.7.0->sumy) (3.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\basak\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk>=3.0.2->sumy) (0.4.6)\n",
      "Building wheels for collected packages: breadability, docopt, pycountry\n",
      "  Building wheel for breadability (setup.py): started\n",
      "  Building wheel for breadability (setup.py): finished with status 'done'\n",
      "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21741 sha256=12c9e0e6ef0a83ee187cc851ad4255092b798d886c315d169c7ffdeb86eb1bbd\n",
      "  Stored in directory: C:\\Users\\basak\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-kr_igvr0\\wheels\\4d\\57\\58\\7e3d7fedf51fe248b7fcee3df6945ae28638e22cddf01eb92b\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13775 sha256=1a391ab7b5a90ea51cce083f7657480d3b2f05e43ee7c58cbd038e25f7bd53c2\n",
      "  Stored in directory: C:\\Users\\basak\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-kr_igvr0\\wheels\\1a\\b0\\8c\\4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
      "  Building wheel for pycountry (pyproject.toml): started\n",
      "  Building wheel for pycountry (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pycountry: filename=pycountry-22.3.5-py2.py3-none-any.whl size=10681895 sha256=d6bfe0a9e475f5d5f3e9884d742d97968dccc0af96ca9c466e515202c4fc6f8d\n",
      "  Stored in directory: C:\\Users\\basak\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-kr_igvr0\\wheels\\cd\\29\\8b\\617685ed7942656b36efb06ff9247dbe832e3f4f7724fffc09\n",
      "Successfully built breadability docopt pycountry\n",
      "Installing collected packages: lxml, docopt, chardet, pycountry, breadability, sumy\n",
      "Successfully installed breadability-0.1.20 chardet-5.1.0 docopt-0.6.2 lxml-4.9.2 pycountry-22.3.5 sumy-0.11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\basak\\AppData\\Local\\Programs\\Python\\Python311\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"data\\bbc_text_cls.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = TextRankSummarizer()\n",
    "parser = PlaintextParser.from_string(doc, Tokenizer(\"english\"))\n",
    "summary = summarizer(parser.document, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It hopes to increase subscribers by offering the online service free to TimeWarner internet customers and will try to sign up AOL's existing customers for high-speed broadband.\n",
      "But its film division saw profits slump 27% to $284m, helped by box-office flops Alexander and Catwoman, a sharp contrast to year-earlier, when the third and final film in the Lord of the Rings trilogy boosted results.\n",
      "It intends to adjust the way it accounts for a deal with German music publisher Bertelsmann's purchase of a stake in AOL Europe, which it had reported as advertising revenue.\n"
     ]
    }
   ],
   "source": [
    "for sentence in summary:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.\n",
      "Time Warner said on Friday that it now owns 8% of search-engine Google.\n",
      "For the full-year, TimeWarner posted a profit of $3.36bn, up 27% from its 2003 performance, while revenues grew 6.4% to $42.09bn.\n"
     ]
    }
   ],
   "source": [
    "summarizer = LsaSummarizer()\n",
    "summary = summarizer(parser.document, 3)\n",
    "for sentence in summary:\n",
    "    print(sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we could use gensim to summarize the text. Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora. Target audience is the natural language processing (NLP) and information retrieval (IR) community.<br>\n",
    "\n",
    "\n",
    "Parameters:<br>\n",
    "text (str) – Input text.<br>\n",
    "ratio (float) – How much the text should be summarized. If 0.2 the summarized text will contain 20% of the original sentences.<br>\n",
    "word_count (int) – How many words the summary should contain. If both ratio and word_count are set, the ratio will be ignored.<br>\n",
    "split (bool) – If True, the function will return a list of strings, one for each sentence in the summary. If False, it will return a single string.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gensim.summarization.summarizer import summarize<br>\n",
    "#summary = summarize(doc, ratio=0.1)\n",
    "#print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "48568052eec3e48bec5163d0241420ad0cb3738ffc0c56fbb083086b8e264995"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
